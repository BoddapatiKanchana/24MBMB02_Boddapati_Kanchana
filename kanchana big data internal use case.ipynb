{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f49f64d5-ab3c-43cc-bc18-7e8afb888db8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers: 75\nHotels: 75\nBookings: 75\n+----------+-----------+--------------+--------+-----------------+------------+--------+\n|booking_id|customer_id|customer_name |hotel_id|hotel_name       |booking_date|amount  |\n+----------+-----------+--------------+--------+-----------------+------------+--------+\n|1         |36         |Ishaan Reddy  |72      |Garden Suites 72 |2024-01-11  |1875.0  |\n|2         |63         |Anika Sharma  |72      |Garden Suites 72 |2024-09-03  |6000.0  |\n|3         |12         |Ishaan Iyer   |38      |Lake Retreat 38  |2024-08-14  |15092.77|\n|4         |55         |Saanvi Das    |71      |Lake Retreat 71  |2024-12-04  |13861.97|\n|5         |25         |Vihaan Joshi  |41      |Fort Residency 41|2024-05-02  |59810.38|\n|6         |68         |Arjun Rao     |37      |Garden Palace 37 |2024-04-12  |47398.56|\n|7         |69         |Aaradhya Joshi|17      |Sky Resort 17    |2024-10-07  |12606.7 |\n|8         |14         |Rohan Reddy   |2       |Sea Residency 2  |2025-08-10  |17067.66|\n|9         |62         |Kabir Patel   |15      |Mountain Lodge 15|2024-03-07  |18226.22|\n|10        |39         |Anika Das     |11      |Grand Haven 11   |2024-09-11  |7414.08 |\n+----------+-----------+--------------+--------+-----------------+------------+--------+\nonly showing top 10 rows\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>hotel_name</th><th>avg_stay_nights</th></tr></thead><tbody><tr><td>Grand Palace 29</td><td>7.0</td></tr><tr><td>Garden Palace 37</td><td>7.0</td></tr><tr><td>Metro Retreat 8</td><td>7.0</td></tr><tr><td>City Suites 10</td><td>7.0</td></tr><tr><td>City Lodge 43</td><td>7.0</td></tr><tr><td>Grand Retreat 65</td><td>7.0</td></tr><tr><td>Mountain Inn 6</td><td>6.5</td></tr><tr><td>Fort Residency 41</td><td>6.0</td></tr><tr><td>Mountain Resort 59</td><td>6.0</td></tr><tr><td>Sea Palace 62</td><td>6.0</td></tr><tr><td>Mountain Palace 66</td><td>6.0</td></tr><tr><td>Fort Residency 36</td><td>5.5</td></tr><tr><td>Garden Suites 72</td><td>5.333333333333333</td></tr><tr><td>Lake Haven 14</td><td>5.0</td></tr><tr><td>Sun Hotel 60</td><td>5.0</td></tr><tr><td>Mountain Lodge 15</td><td>5.0</td></tr><tr><td>Garden Hotel 35</td><td>5.0</td></tr><tr><td>City Hotel 7</td><td>5.0</td></tr><tr><td>City Suites 58</td><td>5.0</td></tr><tr><td>City Plaza 49</td><td>5.0</td></tr><tr><td>Lake Plaza 61</td><td>4.666666666666667</td></tr><tr><td>Mountain Plaza 13</td><td>4.666666666666667</td></tr><tr><td>City Palace 26</td><td>4.0</td></tr><tr><td>Grand Haven 54</td><td>4.0</td></tr><tr><td>Lake Lodge 44</td><td>4.0</td></tr><tr><td>Fort Residency 70</td><td>4.0</td></tr><tr><td>Grand Lodge 67</td><td>4.0</td></tr><tr><td>Mountain Hotel 4</td><td>3.5</td></tr><tr><td>Sea Haven 21</td><td>3.0</td></tr><tr><td>City Suites 45</td><td>3.0</td></tr><tr><td>Fort Residency 30</td><td>3.0</td></tr><tr><td>Lake Retreat 71</td><td>3.0</td></tr><tr><td>Lake Retreat 38</td><td>3.0</td></tr><tr><td>Sea Residency 2</td><td>3.0</td></tr><tr><td>Sun Resort 39</td><td>3.0</td></tr><tr><td>Metro Haven 18</td><td>3.0</td></tr><tr><td>Sea Inn 3</td><td>3.0</td></tr><tr><td>Sea Palace 40</td><td>3.0</td></tr><tr><td>Sea Resort 57</td><td>2.5</td></tr><tr><td>Sky Plaza 64</td><td>2.0</td></tr><tr><td>Garden Plaza 33</td><td>2.0</td></tr><tr><td>Metro Lodge 42</td><td>2.0</td></tr><tr><td>Sky Resort 17</td><td>2.0</td></tr><tr><td>Grand Haven 11</td><td>1.5</td></tr><tr><td>Sky Lodge 73</td><td>1.5</td></tr><tr><td>Mountain Residency 20</td><td>1.0</td></tr><tr><td>Garden Retreat 52</td><td>1.0</td></tr><tr><td>Fort Residency 34</td><td>1.0</td></tr><tr><td>Mountain Residency 68</td><td>1.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Grand Palace 29",
         7.0
        ],
        [
         "Garden Palace 37",
         7.0
        ],
        [
         "Metro Retreat 8",
         7.0
        ],
        [
         "City Suites 10",
         7.0
        ],
        [
         "City Lodge 43",
         7.0
        ],
        [
         "Grand Retreat 65",
         7.0
        ],
        [
         "Mountain Inn 6",
         6.5
        ],
        [
         "Fort Residency 41",
         6.0
        ],
        [
         "Mountain Resort 59",
         6.0
        ],
        [
         "Sea Palace 62",
         6.0
        ],
        [
         "Mountain Palace 66",
         6.0
        ],
        [
         "Fort Residency 36",
         5.5
        ],
        [
         "Garden Suites 72",
         5.333333333333333
        ],
        [
         "Lake Haven 14",
         5.0
        ],
        [
         "Sun Hotel 60",
         5.0
        ],
        [
         "Mountain Lodge 15",
         5.0
        ],
        [
         "Garden Hotel 35",
         5.0
        ],
        [
         "City Hotel 7",
         5.0
        ],
        [
         "City Suites 58",
         5.0
        ],
        [
         "City Plaza 49",
         5.0
        ],
        [
         "Lake Plaza 61",
         4.666666666666667
        ],
        [
         "Mountain Plaza 13",
         4.666666666666667
        ],
        [
         "City Palace 26",
         4.0
        ],
        [
         "Grand Haven 54",
         4.0
        ],
        [
         "Lake Lodge 44",
         4.0
        ],
        [
         "Fort Residency 70",
         4.0
        ],
        [
         "Grand Lodge 67",
         4.0
        ],
        [
         "Mountain Hotel 4",
         3.5
        ],
        [
         "Sea Haven 21",
         3.0
        ],
        [
         "City Suites 45",
         3.0
        ],
        [
         "Fort Residency 30",
         3.0
        ],
        [
         "Lake Retreat 71",
         3.0
        ],
        [
         "Lake Retreat 38",
         3.0
        ],
        [
         "Sea Residency 2",
         3.0
        ],
        [
         "Sun Resort 39",
         3.0
        ],
        [
         "Metro Haven 18",
         3.0
        ],
        [
         "Sea Inn 3",
         3.0
        ],
        [
         "Sea Palace 40",
         3.0
        ],
        [
         "Sea Resort 57",
         2.5
        ],
        [
         "Sky Plaza 64",
         2.0
        ],
        [
         "Garden Plaza 33",
         2.0
        ],
        [
         "Metro Lodge 42",
         2.0
        ],
        [
         "Sky Resort 17",
         2.0
        ],
        [
         "Grand Haven 11",
         1.5
        ],
        [
         "Sky Lodge 73",
         1.5
        ],
        [
         "Mountain Residency 20",
         1.0
        ],
        [
         "Garden Retreat 52",
         1.0
        ],
        [
         "Fort Residency 34",
         1.0
        ],
        [
         "Mountain Residency 68",
         1.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "hotel_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "avg_stay_nights",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBTVEVQIDEg4oCUIEltcG9ydHMKZnJvbSBweXNwYXJrLnNxbC50eXBlcyBpbXBvcnQgKgpmcm9tIHB5c3Bhcmsuc3FsLmZ1bmN0aW9ucyBpbXBvcnQgKgoKCiMgTG9hZCB0YWJsZXMgZGlyZWN0bHkgZnJvbSBjYXRhbG9nIChkZWZhdWx0IHNjaGVtYSkKY3VzdG9tZXJzID0gc3BhcmsudGFibGUoIndvcmtzcGFjZS5kZWZhdWx0LmN1c3RvbWVyc183NSIpCmhvdGVscyAgICA9IHNwYXJrLnRhYmxlKCJ3b3Jrc3BhY2UuZGVmYXVsdC5ob3RlbHNfNzUiKQpib29raW5ncyAgPSBzcGFyay50YWJsZSgid29ya3NwYWNlLmRlZmF1bHQuYm9va2luZ3NfNzUiKQoKIyBRdWljayBzYW5pdHkgY2hlY2sKcHJpbnQoIkN1c3RvbWVyczoiLCBjdXN0b21lcnMuY291bnQoKSkKcHJpbnQoIkhvdGVsczoiLCBob3RlbHMuY291bnQoKSkKcHJpbnQoIkJvb2tpbmdzOiIsIGJvb2tpbmdzLmNvdW50KCkpCgoKIyBKb2luIGJvb2tpbmdzIHdpdGggY3VzdG9tZXJzIGFuZCBob3RlbHMKam9pbmVkX2RmID0gKAogICAgYm9va2luZ3MKICAgIC5qb2luKGN1c3RvbWVycywgImN1c3RvbWVyX2lkIikKICAgIC5qb2luKGhvdGVscywgImhvdGVsX2lkIikKKQoKIyBQcmV2aWV3IGpvaW5lZCBkYXRhCmpvaW5lZF9kZi5zZWxlY3QoCiAgICAiYm9va2luZ19pZCIsCiAgICAiY3VzdG9tZXJfaWQiLAogICAgImN1c3RvbWVyX25hbWUiLAogICAgImhvdGVsX2lkIiwKICAgICJob3RlbF9uYW1lIiwKICAgICJib29raW5nX2RhdGUiLAogICAgImFtb3VudCIKKS5zaG93KDEwLCB0cnVuY2F0ZT1GYWxzZSkKCmZyb20gcHlzcGFyay5zcWwuZnVuY3Rpb25zIGltcG9ydCBjb2wsIGF2ZwoKIyBDb252ZXJ0IHN0YXlfbmlnaHRzIGNvbHVtbiB0byBpbnRlZ2VyIGFuZCB0aGVuIGNhbGN1bGF0ZSBhdmVyYWdlCmF2Z19kdXJhdGlvbiA9ICgKICAgIGJvb2tpbmdzCiAgICAud2l0aENvbHVtbigic3RheV9uaWdodHNfaW50IiwgY29sKCJzdGF5X25pZ2h0cyIpLmNhc3QoImludCIpKSAgIyBjYXN0IHRvIGludAogICAgLmdyb3VwQnkoImhvdGVsX2lkIikKICAgIC5hZ2coYXZnKCJzdGF5X25pZ2h0c19pbnQiKS5hbGlhcygiYXZnX3N0YXlfbmlnaHRzIikpCiAgICAuam9pbihob3RlbHMsICJob3RlbF9pZCIpCiAgICAuc2VsZWN0KCJob3RlbF9uYW1lIiwgImF2Z19zdGF5X25pZ2h0cyIpCiAgICAub3JkZXJCeShjb2woImF2Z19zdGF5X25pZ2h0cyIpLmRlc2MoKSkKKQoKZGlzcGxheShhdmdfZHVyYXRpb24p\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView263e573\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView263e573\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView263e573\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView263e573) SELECT `hotel_name`,SUM(`avg_stay_nights`) `column_7e652f6e138`,`hotel_name` FROM q GROUP BY `hotel_name`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView263e573\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Avg Stay by Hotel Name1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "avg_stay_nights",
             "id": "column_7e652f6e169"
            }
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "histogram",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numBins": 10,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_7e652f6e138": {
             "type": "histogram",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "35383276-5994-4d19-8778-a159d0ba6aa8",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 2.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "avg_stay_nights_BIN",
           "type": "column"
          }
         ],
         "selects": [
          {
           "alias": "avg_stay_nights_BIN",
           "args": [
            {
             "column": "avg_stay_nights",
             "type": "column"
            },
            {
             "number": 10,
             "type": "number"
            }
           ],
           "function": "BIN",
           "type": "function"
          },
          {
           "alias": "avg_stay_nights_BIN_LOWER_BOUND",
           "args": [
            {
             "column": "avg_stay_nights",
             "type": "column"
            },
            {
             "number": 10,
             "type": "number"
            }
           ],
           "function": "BIN_LOWER_BOUND",
           "type": "function"
          },
          {
           "alias": "avg_stay_nights_BIN_STEP",
           "args": [
            {
             "column": "avg_stay_nights",
             "type": "column"
            },
            {
             "number": 10,
             "type": "number"
            }
           ],
           "function": "BIN_STEP",
           "type": "function"
          },
          {
           "alias": "COUNT",
           "args": [
            {
             "column": "avg_stay_nights",
             "type": "column"
            }
           ],
           "function": "COUNT",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": [],
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>month</th><th>total_bookings</th></tr></thead><tbody><tr><td>1</td><td>8</td></tr><tr><td>2</td><td>10</td></tr><tr><td>3</td><td>7</td></tr><tr><td>4</td><td>7</td></tr><tr><td>5</td><td>4</td></tr><tr><td>6</td><td>4</td></tr><tr><td>7</td><td>7</td></tr><tr><td>8</td><td>9</td></tr><tr><td>9</td><td>7</td></tr><tr><td>10</td><td>4</td></tr><tr><td>11</td><td>5</td></tr><tr><td>12</td><td>3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         8
        ],
        [
         2,
         10
        ],
        [
         3,
         7
        ],
        [
         4,
         7
        ],
        [
         5,
         4
        ],
        [
         6,
         4
        ],
        [
         7,
         7
        ],
        [
         8,
         9
        ],
        [
         9,
         7
        ],
        [
         10,
         4
        ],
        [
         11,
         5
        ],
        [
         12,
         3
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "month",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "total_bookings",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": {
        "addedWidgets": {},
        "arguments": {},
        "datasetInfos": [],
        "jupyterProps": {
         "ename": "AnalysisException",
         "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `location` cannot be resolved. Did you mean one of the following? [`avg_stay_duration`, `workspace`.`default`.`hotels_75`.`city`, `workspace`.`default`.`hotels_75`.`country`, `workspace`.`default`.`bookings_75`.`hotel_id`, `workspace`.`default`.`hotels_75`.`hotel_name`]. SQLSTATE: 42703;\n'Project ['location]\n+- Project [hotel_id#19197L, avg_stay_duration#19132, hotel_name#19209, city#19210, country#19211, star_rating#19212L, rooms_count#19213L]\n   +- Join Inner, (hotel_id#19197L = hotel_id#19208L)\n      :- Aggregate [hotel_id#19197L], [hotel_id#19197L, avg(stay_nights_int#19214) AS avg_stay_duration#19132]\n      :  +- Project [booking_id#19195L, customer_id#19196L, hotel_id#19197L, booking_date#19198, checkin_date#19199, checkout_date#19200, stay_nights#19201L, room_type#19202, booking_channel#19203, payment_method#19204, status#19205, amount#19206, lead_time_days#19207L, cast(stay_nights#19201L as int) AS stay_nights_int#19214]\n      :     +- SubqueryAlias workspace.default.bookings_75\n      :        +- Relation workspace.default.bookings_75[booking_id#19195L,customer_id#19196L,hotel_id#19197L,booking_date#19198,checkin_date#19199,checkout_date#19200,stay_nights#19201L,room_type#19202,booking_channel#19203,payment_method#19204,status#19205,amount#19206,lead_time_days#19207L] parquet\n      +- SubqueryAlias workspace.default.hotels_75\n         +- Relation workspace.default.hotels_75[hotel_id#19208L,hotel_name#19209,city#19210,country#19211,star_rating#19212L,rooms_count#19213L] parquet\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:593)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:476)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:476)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:476)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:476)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:318)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:318)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:289)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:499)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:274)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:95)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:153)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:261)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:261)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:499)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:390)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:248)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:390)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:555)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:555)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:548)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:327)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:95)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:153)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:86)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:646)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:810)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:155)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:136)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$6(QueryExecution.scala:810)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1449)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:803)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:800)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:800)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:789)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:799)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:798)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:309)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:308)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1686)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1747)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:369)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:288)\n\tat org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:405)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.getDataFrameWithoutExecuting$1(SparkConnectAnalyzeHandler.scala:104)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:181)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:78)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:70)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:70)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:55)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:263)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:54)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:114)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:801)\n\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:387)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:356)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:378)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:356)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:574)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:355)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:354)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:386)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:347)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:387)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"
        },
        "metadata": {},
        "removedWidgets": [],
        "sqlProps": {
         "breakingChangeInfo": null,
         "errorClass": "UNRESOLVED_COLUMN.WITH_SUGGESTION",
         "pysparkCallSite": "<command-6246054675326419>, line 90 in cell [46]",
         "pysparkFragment": "col",
         "pysparkSummary": "",
         "sqlState": "42703",
         "stackTrace": "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:593)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:476)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:476)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:476)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:476)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:318)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:318)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:289)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:499)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:274)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:95)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:153)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:261)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:261)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:499)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:390)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:248)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:390)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:555)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:555)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:548)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:327)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:95)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:153)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:86)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:646)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:810)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:155)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:136)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$6(QueryExecution.scala:810)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1449)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:803)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:800)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:800)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:789)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:799)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:798)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:309)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:308)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1686)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1747)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:369)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:288)\n\tat org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:405)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.getDataFrameWithoutExecuting$1(SparkConnectAnalyzeHandler.scala:104)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:181)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:78)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:70)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:70)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:55)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:263)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:54)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:114)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:801)\n\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:387)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:356)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:378)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:356)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:574)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:355)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:354)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:386)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:347)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:387)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)",
         "startIndex": null,
         "stopIndex": null
        },
        "stackFrames": [
         "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
         "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
         "File \u001B[0;32m<command-6246054675326419>, line 16\u001B[0m\n\u001B[1;32m     12\u001B[0m     exec(base64\u001B[38;5;241m.\u001B[39mstandard_b64decode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;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\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdecode())\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;66;03m# run user code\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m     __backend_agg_user_code_fn()\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;66;03m#reset display function\u001B[39;00m\n\u001B[1;32m     19\u001B[0m     display \u001B[38;5;241m=\u001B[39m __backend_agg_display_orig\n",
         "File \u001B[0;32m<command-6246054675326419>, line 12\u001B[0m, in \u001B[0;36m__backend_agg_user_code_fn\u001B[0;34m()\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__backend_agg_user_code_fn\u001B[39m():\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m     exec(base64\u001B[38;5;241m.\u001B[39mstandard_b64decode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;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\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdecode())\n",
         "File \u001B[0;32m<string>:90\u001B[0m\n",
         "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:560\u001B[0m, in \u001B[0;36mDataFrame.groupBy\u001B[0;34m(self, *cols)\u001B[0m\n\u001B[1;32m    558\u001B[0m     _cols\u001B[38;5;241m.\u001B[39mappend(c)\n\u001B[1;32m    559\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(c, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m--> 560\u001B[0m     _cols\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m[c])\n\u001B[1;32m    561\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(c, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(c, \u001B[38;5;28mbool\u001B[39m):\n\u001B[1;32m    562\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m c \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
         "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1885\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m   1882\u001B[0m             \u001B[38;5;66;03m# Try best to verify the column name with cached schema\u001B[39;00m\n\u001B[1;32m   1883\u001B[0m             \u001B[38;5;66;03m# If fails, fall back to the server side validation\u001B[39;00m\n\u001B[1;32m   1884\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m verify_col_name(item, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_schema):\n\u001B[0;32m-> 1885\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mselect(item)\u001B[38;5;241m.\u001B[39misLocal()\n\u001B[1;32m   1887\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_col(item)\n\u001B[1;32m   1888\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(item, Column):\n",
         "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:2058\u001B[0m, in \u001B[0;36mDataFrame.isLocal\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2055\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mcache\n\u001B[1;32m   2056\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21misLocal\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[1;32m   2057\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n\u001B[0;32m-> 2058\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39m_analyze(method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis_local\u001B[39m\u001B[38;5;124m\"\u001B[39m, plan\u001B[38;5;241m=\u001B[39mquery)\u001B[38;5;241m.\u001B[39mis_local\n\u001B[1;32m   2059\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2060\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
         "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1743\u001B[0m, in \u001B[0;36mSparkConnectClient._analyze\u001B[0;34m(self, method, **kwargs)\u001B[0m\n\u001B[1;32m   1741\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid state during retry exception handling.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1743\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
         "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2266\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2264\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2265\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2266\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2267\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   2268\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
         "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2377\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2363\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2364\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPython versions in the Spark Connect client and server are different. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2365\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo execute user-defined functions, client and server should have the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2373\u001B[0m                         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msqlState\u001B[39m\u001B[38;5;124m\"\u001B[39m, default\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE),\n\u001B[1;32m   2374\u001B[0m                 ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2375\u001B[0m             \u001B[38;5;66;03m# END-EDGE\u001B[39;00m\n\u001B[0;32m-> 2377\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2378\u001B[0m                 info,\n\u001B[1;32m   2379\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2380\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2381\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2382\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2384\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2385\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2386\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2387\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2388\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
         "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `location` cannot be resolved. Did you mean one of the following? [`avg_stay_duration`, `workspace`.`default`.`hotels_75`.`city`, `workspace`.`default`.`hotels_75`.`country`, `workspace`.`default`.`bookings_75`.`hotel_id`, `workspace`.`default`.`hotels_75`.`hotel_name`]. SQLSTATE: 42703;\n'Project ['location]\n+- Project [hotel_id#19197L, avg_stay_duration#19132, hotel_name#19209, city#19210, country#19211, star_rating#19212L, rooms_count#19213L]\n   +- Join Inner, (hotel_id#19197L = hotel_id#19208L)\n      :- Aggregate [hotel_id#19197L], [hotel_id#19197L, avg(stay_nights_int#19214) AS avg_stay_duration#19132]\n      :  +- Project [booking_id#19195L, customer_id#19196L, hotel_id#19197L, booking_date#19198, checkin_date#19199, checkout_date#19200, stay_nights#19201L, room_type#19202, booking_channel#19203, payment_method#19204, status#19205, amount#19206, lead_time_days#19207L, cast(stay_nights#19201L as int) AS stay_nights_int#19214]\n      :     +- SubqueryAlias workspace.default.bookings_75\n      :        +- Relation workspace.default.bookings_75[booking_id#19195L,customer_id#19196L,hotel_id#19197L,booking_date#19198,checkin_date#19199,checkout_date#19200,stay_nights#19201L,room_type#19202,booking_channel#19203,payment_method#19204,status#19205,amount#19206,lead_time_days#19207L] parquet\n      +- SubqueryAlias workspace.default.hotels_75\n         +- Relation workspace.default.hotels_75[hotel_id#19208L,hotel_name#19209,city#19210,country#19211,star_rating#19212L,rooms_count#19213L] parquet\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:593)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10(CheckAnalysis.scala:491)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$10$adapted(CheckAnalysis.scala:476)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9(CheckAnalysis.scala:476)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$9$adapted(CheckAnalysis.scala:476)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:476)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:318)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:311)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:318)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:289)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:499)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:274)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:95)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:153)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:261)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:261)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:499)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:390)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:248)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:390)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:98)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:135)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:555)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:555)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:548)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:327)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:95)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:153)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:86)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:646)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:810)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:155)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:136)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$6(QueryExecution.scala:810)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1449)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:803)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:800)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:800)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:789)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:799)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:798)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:309)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:308)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1686)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1747)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:369)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:288)\n\tat org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:405)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.getDataFrameWithoutExecuting$1(SparkConnectAnalyzeHandler.scala:104)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.process(SparkConnectAnalyzeHandler.scala:181)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3(SparkConnectAnalyzeHandler.scala:78)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$3$adapted(SparkConnectAnalyzeHandler.scala:70)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:466)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:466)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:465)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1(SparkConnectAnalyzeHandler.scala:70)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.$anonfun$handle$1$adapted(SparkConnectAnalyzeHandler.scala:55)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:263)\n\tat org.apache.spark.sql.connect.service.SparkConnectAnalyzeHandler.handle(SparkConnectAnalyzeHandler.scala:54)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.analyzePlan(SparkConnectService.scala:114)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:801)\n\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:387)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:356)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:18)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:42)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:378)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:356)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:574)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:355)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:291)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:287)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:354)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:386)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:347)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:387)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)"
        ],
        "type": "baseError"
       },
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"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\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 1:\n        # create a temp view\n        if type(__backend_agg_dfs[1]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[1].to_spark().createOrReplaceTempView(\"DatabricksView7a3d6ab\")\n        elif type(__backend_agg_dfs[1]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[1], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[1]).createOrReplaceTempView(\"DatabricksView7a3d6ab\")\n        else:\n            __backend_agg_dfs[1].createOrReplaceTempView(\"DatabricksView7a3d6ab\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView7a3d6ab) SELECT `month`,SUM(`total_bookings`) `column_7e652f6e148` FROM q GROUP BY `month`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView7a3d6ab\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Seasonal Trend1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "month",
             "id": "column_7e652f6e147"
            },
            "y": [
             {
              "column": "total_bookings",
              "id": "column_7e652f6e148",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_7e652f6e148": {
             "name": "total_bookings",
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "1a04459c-39f7-4b7f-8088-9bd3cc30ba56",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 2.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "month",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "month",
           "type": "column"
          },
          {
           "alias": "column_7e652f6e148",
           "args": [
            {
             "column": "total_bookings",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 1,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": [],
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>hotel_name</th><th>total_revenue</th></tr></thead><tbody><tr><td>Metro Retreat 8</td><td>87169.79</td></tr><tr><td>Mountain Plaza 13</td><td>83667.72</td></tr><tr><td>Lake Plaza 61</td><td>76087.57</td></tr><tr><td>Lake Haven 14</td><td>75945.73000000001</td></tr><tr><td>Mountain Palace 66</td><td>72265.53</td></tr><tr><td>Garden Suites 72</td><td>61611.07</td></tr><tr><td>Fort Residency 41</td><td>59810.38</td></tr><tr><td>Sea Resort 57</td><td>49492.479999999996</td></tr><tr><td>City Plaza 49</td><td>48459.6</td></tr><tr><td>Sea Palace 40</td><td>47646.46</td></tr><tr><td>Garden Palace 37</td><td>47398.56</td></tr><tr><td>Lake Lodge 44</td><td>41320.86</td></tr><tr><td>Fort Residency 36</td><td>38290.7</td></tr><tr><td>Mountain Hotel 4</td><td>37634.95</td></tr><tr><td>Fort Residency 30</td><td>37200.66</td></tr><tr><td>Lake Retreat 38</td><td>34026.47</td></tr><tr><td>Mountain Inn 6</td><td>33209.38</td></tr><tr><td>Sea Haven 21</td><td>32568.24</td></tr><tr><td>Grand Haven 11</td><td>32191.39</td></tr><tr><td>Sky Resort 17</td><td>28123.95</td></tr><tr><td>City Palace 26</td><td>27017.76</td></tr><tr><td>Garden Plaza 33</td><td>25808.019999999997</td></tr><tr><td>City Hotel 7</td><td>24910.95</td></tr><tr><td>Sun Resort 39</td><td>24225.62</td></tr><tr><td>Fort Residency 70</td><td>23667.81</td></tr><tr><td>Mountain Lodge 15</td><td>20926.22</td></tr><tr><td>Sun Hotel 60</td><td>17184.33</td></tr><tr><td>Sea Residency 2</td><td>17067.66</td></tr><tr><td>Lake Retreat 71</td><td>13861.97</td></tr><tr><td>Metro Lodge 42</td><td>9442.2</td></tr><tr><td>Sky Lodge 73</td><td>8624.74</td></tr><tr><td>Mountain Residency 68</td><td>8167.99</td></tr><tr><td>Mountain Resort 59</td><td>7200.0</td></tr><tr><td>Grand Palace 29</td><td>7000.0</td></tr><tr><td>City Lodge 43</td><td>7000.0</td></tr><tr><td>Grand Retreat 65</td><td>5250.0</td></tr><tr><td>City Suites 10</td><td>4375.0</td></tr><tr><td>Garden Retreat 52</td><td>4102.64</td></tr><tr><td>Mountain Residency 20</td><td>3929.1</td></tr><tr><td>Sea Inn 3</td><td>3600.0</td></tr><tr><td>Grand Haven 54</td><td>3400.0</td></tr><tr><td>Garden Hotel 35</td><td>2500.0</td></tr><tr><td>City Suites 45</td><td>1875.0</td></tr><tr><td>Sky Plaza 64</td><td>1600.0</td></tr><tr><td>Grand Lodge 67</td><td>1500.0</td></tr><tr><td>Metro Haven 18</td><td>1500.0</td></tr><tr><td>Fort Residency 34</td><td>500.0</td></tr><tr><td>City Suites 58</td><td>0.0</td></tr><tr><td>Sea Palace 62</td><td>0.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Metro Retreat 8",
         87169.79
        ],
        [
         "Mountain Plaza 13",
         83667.72
        ],
        [
         "Lake Plaza 61",
         76087.57
        ],
        [
         "Lake Haven 14",
         75945.73000000001
        ],
        [
         "Mountain Palace 66",
         72265.53
        ],
        [
         "Garden Suites 72",
         61611.07
        ],
        [
         "Fort Residency 41",
         59810.38
        ],
        [
         "Sea Resort 57",
         49492.479999999996
        ],
        [
         "City Plaza 49",
         48459.6
        ],
        [
         "Sea Palace 40",
         47646.46
        ],
        [
         "Garden Palace 37",
         47398.56
        ],
        [
         "Lake Lodge 44",
         41320.86
        ],
        [
         "Fort Residency 36",
         38290.7
        ],
        [
         "Mountain Hotel 4",
         37634.95
        ],
        [
         "Fort Residency 30",
         37200.66
        ],
        [
         "Lake Retreat 38",
         34026.47
        ],
        [
         "Mountain Inn 6",
         33209.38
        ],
        [
         "Sea Haven 21",
         32568.24
        ],
        [
         "Grand Haven 11",
         32191.39
        ],
        [
         "Sky Resort 17",
         28123.95
        ],
        [
         "City Palace 26",
         27017.76
        ],
        [
         "Garden Plaza 33",
         25808.019999999997
        ],
        [
         "City Hotel 7",
         24910.95
        ],
        [
         "Sun Resort 39",
         24225.62
        ],
        [
         "Fort Residency 70",
         23667.81
        ],
        [
         "Mountain Lodge 15",
         20926.22
        ],
        [
         "Sun Hotel 60",
         17184.33
        ],
        [
         "Sea Residency 2",
         17067.66
        ],
        [
         "Lake Retreat 71",
         13861.97
        ],
        [
         "Metro Lodge 42",
         9442.2
        ],
        [
         "Sky Lodge 73",
         8624.74
        ],
        [
         "Mountain Residency 68",
         8167.99
        ],
        [
         "Mountain Resort 59",
         7200.0
        ],
        [
         "Grand Palace 29",
         7000.0
        ],
        [
         "City Lodge 43",
         7000.0
        ],
        [
         "Grand Retreat 65",
         5250.0
        ],
        [
         "City Suites 10",
         4375.0
        ],
        [
         "Garden Retreat 52",
         4102.64
        ],
        [
         "Mountain Residency 20",
         3929.1
        ],
        [
         "Sea Inn 3",
         3600.0
        ],
        [
         "Grand Haven 54",
         3400.0
        ],
        [
         "Garden Hotel 35",
         2500.0
        ],
        [
         "City Suites 45",
         1875.0
        ],
        [
         "Sky Plaza 64",
         1600.0
        ],
        [
         "Grand Lodge 67",
         1500.0
        ],
        [
         "Metro Haven 18",
         1500.0
        ],
        [
         "Fort Residency 34",
         500.0
        ],
        [
         "City Suites 58",
         0.0
        ],
        [
         "Sea Palace 62",
         0.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "hotel_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_revenue",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"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\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 2:\n        # create a temp view\n        if type(__backend_agg_dfs[2]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[2].to_spark().createOrReplaceTempView(\"DatabricksViewc47cf16\")\n        elif type(__backend_agg_dfs[2]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[2], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[2]).createOrReplaceTempView(\"DatabricksViewc47cf16\")\n        else:\n            __backend_agg_dfs[2].createOrReplaceTempView(\"DatabricksViewc47cf16\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewc47cf16) SELECT `hotel_name`,SUM(`total_revenue`) `column_7e652f6e156` FROM q GROUP BY `hotel_name`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewc47cf16\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Hotels with Highest Revenue1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "hotel_name",
             "id": "column_7e652f6e154"
            },
            "y": [
             {
              "column": "total_revenue",
              "id": "column_7e652f6e156",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "line",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_7e652f6e156": {
             "type": "line",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "05fce692-10b1-453e-95ba-f47bf9f7bbe3",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 2.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "hotel_name",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "hotel_name",
           "type": "column"
          },
          {
           "alias": "column_7e652f6e156",
           "args": [
            {
             "column": "total_revenue",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 2,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": [],
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>city</th><th>avg_stay_by_city</th></tr></thead><tbody><tr><td>Mumbai</td><td>5.833333333333334</td></tr><tr><td>Chennai</td><td>4.805555555555555</td></tr><tr><td>Goa</td><td>4.75</td></tr><tr><td>Delhi</td><td>4.5</td></tr><tr><td>Kochi</td><td>4.5</td></tr><tr><td>Ahmedabad</td><td>4.333333333333334</td></tr><tr><td>Jaipur</td><td>4.333333333333333</td></tr><tr><td>Pune</td><td>4.0</td></tr><tr><td>Kolkata</td><td>3.5555555555555554</td></tr><tr><td>Bengaluru</td><td>3.375</td></tr><tr><td>Hyderabad</td><td>2.4</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Mumbai",
         5.833333333333334
        ],
        [
         "Chennai",
         4.805555555555555
        ],
        [
         "Goa",
         4.75
        ],
        [
         "Delhi",
         4.5
        ],
        [
         "Kochi",
         4.5
        ],
        [
         "Ahmedabad",
         4.333333333333334
        ],
        [
         "Jaipur",
         4.333333333333333
        ],
        [
         "Pune",
         4.0
        ],
        [
         "Kolkata",
         3.5555555555555554
        ],
        [
         "Bengaluru",
         3.375
        ],
        [
         "Hyderabad",
         2.4
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "avg_stay_by_city",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"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\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 3:\n        # create a temp view\n        if type(__backend_agg_dfs[3]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[3].to_spark().createOrReplaceTempView(\"DatabricksView468e786\")\n        elif type(__backend_agg_dfs[3]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[3], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[3]).createOrReplaceTempView(\"DatabricksView468e786\")\n        else:\n            __backend_agg_dfs[3].createOrReplaceTempView(\"DatabricksView468e786\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView468e786) SELECT `city`,AVG(`avg_stay_by_city`) `column_7e652f6e159` FROM q GROUP BY `city`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView468e786\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Average stay by Location1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "city",
             "id": "column_7e652f6e158"
            },
            "y": [
             {
              "column": "avg_stay_by_city",
              "id": "column_7e652f6e159",
              "transform": "AVG"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_7e652f6e159": {
             "name": "avg_stay_by_city",
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": true,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "ae6740a0-1538-442b-8581-095b9a5baa41",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 2.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "city",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "city",
           "type": "column"
          },
          {
           "alias": "column_7e652f6e159",
           "args": [
            {
             "column": "avg_stay_by_city",
             "type": "column"
            }
           ],
           "function": "AVG",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 3,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": [],
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>hotel_name</th><th>num_cancellations</th></tr></thead><tbody><tr><td>Garden Suites 72</td><td>2</td></tr><tr><td>Grand Haven 54</td><td>2</td></tr><tr><td>Sky Resort 17</td><td>1</td></tr><tr><td>Grand Lodge 67</td><td>1</td></tr><tr><td>City Suites 10</td><td>1</td></tr><tr><td>Garden Hotel 35</td><td>1</td></tr><tr><td>City Palace 26</td><td>1</td></tr><tr><td>Grand Palace 29</td><td>1</td></tr><tr><td>Fort Residency 34</td><td>1</td></tr><tr><td>Sea Inn 3</td><td>1</td></tr><tr><td>Mountain Lodge 15</td><td>1</td></tr><tr><td>City Suites 45</td><td>1</td></tr><tr><td>City Lodge 43</td><td>1</td></tr><tr><td>Metro Haven 18</td><td>1</td></tr><tr><td>Mountain Plaza 13</td><td>1</td></tr><tr><td>Mountain Resort 59</td><td>1</td></tr><tr><td>Grand Retreat 65</td><td>1</td></tr><tr><td>Sky Plaza 64</td><td>1</td></tr><tr><td>City Plaza 49</td><td>1</td></tr><tr><td>Sky Lodge 73</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Garden Suites 72",
         2
        ],
        [
         "Grand Haven 54",
         2
        ],
        [
         "Sky Resort 17",
         1
        ],
        [
         "Grand Lodge 67",
         1
        ],
        [
         "City Suites 10",
         1
        ],
        [
         "Garden Hotel 35",
         1
        ],
        [
         "City Palace 26",
         1
        ],
        [
         "Grand Palace 29",
         1
        ],
        [
         "Fort Residency 34",
         1
        ],
        [
         "Sea Inn 3",
         1
        ],
        [
         "Mountain Lodge 15",
         1
        ],
        [
         "City Suites 45",
         1
        ],
        [
         "City Lodge 43",
         1
        ],
        [
         "Metro Haven 18",
         1
        ],
        [
         "Mountain Plaza 13",
         1
        ],
        [
         "Mountain Resort 59",
         1
        ],
        [
         "Grand Retreat 65",
         1
        ],
        [
         "Sky Plaza 64",
         1
        ],
        [
         "City Plaza 49",
         1
        ],
        [
         "Sky Lodge 73",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "hotel_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "num_cancellations",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"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\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 4:\n        # create a temp view\n        if type(__backend_agg_dfs[4]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[4].to_spark().createOrReplaceTempView(\"DatabricksView3c92fb7\")\n        elif type(__backend_agg_dfs[4]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[4], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[4]).createOrReplaceTempView(\"DatabricksView3c92fb7\")\n        else:\n            __backend_agg_dfs[4].createOrReplaceTempView(\"DatabricksView3c92fb7\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView3c92fb7) SELECT `hotel_name`,SUM(`num_cancellations`) `column_7e652f6e162` FROM q GROUP BY `hotel_name`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView3c92fb7\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "hotel_name",
             "id": "column_7e652f6e161"
            },
            "y": [
             {
              "column": "num_cancellations",
              "id": "column_7e652f6e162",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "pie",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_7e652f6e162": {
             "name": "num_cancellations",
             "type": "pie",
             "yAxis": 0
            }
           },
           "showDataLabels": true,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "ce617aba-fcfa-413a-9af2-3ce0b783b220",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 2.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "hotel_name",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "hotel_name",
           "type": "column"
          },
          {
           "alias": "column_7e652f6e162",
           "args": [
            {
             "column": "num_cancellations",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 4,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": [],
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 1 — Imports\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "# Load tables directly from catalog (default schema)\n",
    "customers = spark.table(\"workspace.default.customers_75\")\n",
    "hotels    = spark.table(\"workspace.default.hotels_75\")\n",
    "bookings  = spark.table(\"workspace.default.bookings_75\")\n",
    "\n",
    "# Quick sanity check\n",
    "print(\"Customers:\", customers.count())\n",
    "print(\"Hotels:\", hotels.count())\n",
    "print(\"Bookings:\", bookings.count())\n",
    "\n",
    "\n",
    "# Join bookings with customers and hotels\n",
    "joined_df = (\n",
    "    bookings\n",
    "    .join(customers, \"customer_id\")\n",
    "    .join(hotels, \"hotel_id\")\n",
    ")\n",
    "\n",
    "# Preview joined data\n",
    "joined_df.select(\n",
    "    \"booking_id\",\n",
    "    \"customer_id\",\n",
    "    \"customer_name\",\n",
    "    \"hotel_id\",\n",
    "    \"hotel_name\",\n",
    "    \"booking_date\",\n",
    "    \"amount\"\n",
    ").show(10, truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "# Convert stay_nights column to integer and then calculate average\n",
    "avg_duration = (\n",
    "    bookings\n",
    "    .withColumn(\"stay_nights_int\", col(\"stay_nights\").cast(\"int\"))  # cast to int\n",
    "    .groupBy(\"hotel_id\")\n",
    "    .agg(avg(\"stay_nights_int\").alias(\"avg_stay_nights\"))\n",
    "    .join(hotels, \"hotel_id\")\n",
    "    .select(\"hotel_name\", \"avg_stay_nights\")\n",
    "    .orderBy(col(\"avg_stay_nights\").desc())\n",
    ")\n",
    "\n",
    "display(avg_duration)\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import month, col, count\n",
    "\n",
    "# Seasonal booking trends by month\n",
    "seasonal_trends = (\n",
    "    bookings\n",
    "    .withColumn(\"month\", month(\"booking_date\"))   # extract month from date\n",
    "    .groupBy(\"month\")\n",
    "    .agg(count(\"*\").alias(\"total_bookings\"))\n",
    "    .orderBy(\"month\")\n",
    ")\n",
    "\n",
    "# Show results as Databricks interactive table\n",
    "display(seasonal_trends)\n",
    "\n",
    "from pyspark.sql.functions import sum, col\n",
    "\n",
    "# Hotels with highest revenue\n",
    "hotel_revenue = (\n",
    "    bookings\n",
    "    .withColumn(\"amount_num\", col(\"amount\").cast(\"double\"))   # ensure amount is numeric\n",
    "    .groupBy(\"hotel_id\")\n",
    "    .agg(sum(\"amount_num\").alias(\"total_revenue\"))\n",
    "    .join(hotels, \"hotel_id\")                                # join to get hotel names\n",
    "    .select(\"hotel_name\", \"total_revenue\")\n",
    "    .orderBy(col(\"total_revenue\").desc())                    # sort by revenue\n",
    ")\n",
    "\n",
    "# Show results as interactive table\n",
    "display(hotel_revenue)\n",
    "\n",
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "# Average stay duration by city\n",
    "avg_stay_city = (\n",
    "    bookings\n",
    "    .withColumn(\"stay_nights_int\", col(\"stay_nights\").cast(\"int\"))   # ensure stay_nights is numeric\n",
    "    .groupBy(\"hotel_id\")\n",
    "    .agg(avg(\"stay_nights_int\").alias(\"avg_stay_duration\"))\n",
    "    .join(hotels, \"hotel_id\")                              # bring city info\n",
    "    .groupBy(\"city\")\n",
    "    .agg(avg(\"avg_stay_duration\").alias(\"avg_stay_by_city\"))\n",
    "    .orderBy(col(\"avg_stay_by_city\").desc())\n",
    ")\n",
    "\n",
    "# Show results as Databricks interactive table\n",
    "display(avg_stay_city)\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col, lower, trim, count\n",
    "\n",
    "# Hotels with highest cancellations (case-insensitive check)\n",
    "cancellations = (\n",
    "    bookings\n",
    "    .withColumn(\"status_clean\", lower(trim(col(\"status\"))))   # clean status column\n",
    "    .filter(col(\"status_clean\") == \"cancelled\")               # filter cancelled\n",
    "    .groupBy(\"hotel_id\")\n",
    "    .agg(count(\"*\").alias(\"num_cancellations\"))               # count cancellations\n",
    "    .join(hotels, \"hotel_id\")                                 # bring hotel names\n",
    "    .select(\"hotel_name\", \"num_cancellations\")\n",
    "    .orderBy(col(\"num_cancellations\").desc())\n",
    ")\n",
    "\n",
    "# Display results as interactive table\n",
    "display(cancellations)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [
    {
     "elements": [
      {
       "dashboardResultIndex": null,
       "elementNUID": "05fce692-10b1-453e-95ba-f47bf9f7bbe3",
       "elementType": "command",
       "guid": "0ede9c49-ea45-4d7d-bdb8-7451b63a4534",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 0,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": null,
       "elementNUID": "35383276-5994-4d19-8778-a159d0ba6aa8",
       "elementType": "command",
       "guid": "22991c71-560a-40d4-84c8-fde4bce5608d",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 6,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": null,
       "elementNUID": "ce617aba-fcfa-413a-9af2-3ce0b783b220",
       "elementType": "command",
       "guid": "5dce0219-de3f-4808-932c-87b89bf51de2",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 12,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": null,
       "elementNUID": "ae6740a0-1538-442b-8581-095b9a5baa41",
       "elementType": "command",
       "guid": "e325edf6-b1a2-466a-bf6f-2413075eb0f6",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 18,
        "z": null
       },
       "resultIndex": null
      },
      {
       "dashboardResultIndex": null,
       "elementNUID": "1a04459c-39f7-4b7f-8088-9bd3cc30ba56",
       "elementType": "command",
       "guid": "e41cc7d5-7808-4a4c-81bc-5a091fb8060b",
       "options": null,
       "position": {
        "height": 6,
        "width": 12,
        "x": 0,
        "y": 24,
        "z": null
       },
       "resultIndex": null
      }
     ],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "d403fbc0-fcfb-4921-b60b-e5c65ecf1f51",
     "origId": 6246054675326418,
     "title": "Travel & Tourism - Booking & Customer in",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "kanchana big data internal use case",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}